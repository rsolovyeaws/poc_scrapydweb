services:
  scrapyd1:
    build:
      context: ./docker/scrapyd
      dockerfile: Dockerfile.scrapyd
    container_name: scrapyd1
    environment:
      - SCRAPYD_EGGS_DIR=/root/.scrapyd/eggs
      - MINIO_ENDPOINT=http://minio:9000          # Points to MinIO container
      - MINIO_ACCESS_KEY=minio_user               # From MinIO service config
      - MINIO_SECRET_KEY=minio_password           # From MinIO service config  
      - MINIO_BUCKET_NAME=scraper-results     
    ports:
      - "6800:6800"
      - "9410:9410"
    volumes:
      - ./docker/shared-eggs:/root/.scrapyd/eggs
      - ./docker/scrapyd/deploy_eggs.sh:/deploy_eggs.sh
      - ./docker/scrapyd/scrapyd_exporter.py:/app/scrapyd_exporter.py
      - ./data/cookies:/data/cookies # Volume for persistent cookies
      - ./logs/scrapyd:/var/lib/scrapyd/logs # Map spider logs to local directory
    entrypoint: /deploy_eggs.sh
    networks:
      - scraper-network

  scrapyd2:
    build:
      context: ./docker/scrapyd
      dockerfile: Dockerfile.scrapyd
    container_name: scrapyd2
    environment:
      - SCRAPYD_EGGS_DIR=/root/.scrapyd/eggs
      - MINIO_ENDPOINT=http://minio:9000          # Points to MinIO container
      - MINIO_ACCESS_KEY=minio_user               # From MinIO service config
      - MINIO_SECRET_KEY=minio_password           # From MinIO service config  
      - MINIO_BUCKET_NAME=scraper-results   
    ports:
      - "6801:6800"
      - "9411:9410"
    volumes:
      - ./docker/shared-eggs:/root/.scrapyd/eggs
      - ./docker/scrapyd/deploy_eggs.sh:/deploy_eggs.sh
      - ./docker/scrapyd/scrapyd_exporter.py:/app/scrapyd_exporter.py
      - ./data/cookies:/data/cookies # Volume for persistent cookies
      - ./logs/scrapyd:/var/lib/scrapyd/logs # Map spider logs to local directory
    entrypoint: /deploy_eggs.sh
    networks:
      - scraper-network

  scrapydweb:
    image: scrapydweb:latest
    container_name: scrapydweb
    depends_on:
      - scrapyd1
      - scrapyd2
    volumes:
      - ./config/scrapydweb_settings_v11.py:/app/scrapydweb_settings_v11.py
    environment:
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
    ports:
      - "5000:5000"
    networks:
      - scraper-network

  # ───────────────────────────────────────
  selenium-hub:
    image: selenium/standalone-chrome:114.0
    container_name: selenium-hub
    ports:
      - "4444:4444"
    networks:
      - scraper-network
    environment:
      - SE_NODE_MAX_SESSIONS=4
      - SE_NODE_SESSION_TIMEOUT=600
      - SE_NODE_OVERRIDE_MAX_SESSIONS=true
      - SE_OPTS=--session-request-timeout 600 --session-retry-interval 2000
      - SE_GRID_MAX_HEAP=2048m
      - GRID_TIMEOUT=600
      - JAVA_OPTS=-Xmx2g
      # Browser shutdown settings to prevent premature session termination
      - DBUS_SESSION_BUS_ADDRESS=/dev/null
    volumes:
      - ./data/cookies:/data/cookies # Volume for persistent cookies
      - /dev/shm:/dev/shm # Improved shared memory for Chrome stability
    shm_size: 4gb
    restart: unless-stopped
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:4444/wd/hub/status" ]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4g
        reservations:
          cpus: '1'
          memory: 2g

  # Database container to store scraping results
  postgres:
    image: postgres:15
    container_name: scraper-db
    restart: always
    environment:
      - POSTGRES_PASSWORD=scraper_password
      - POSTGRES_USER=scraper_user
      - POSTGRES_DB=scraper_data
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init-db:/docker-entrypoint-initdb.d
    networks:
      - scraper-network

  # Optional: Database admin UI
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@example.com
      - PGADMIN_DEFAULT_PASSWORD=admin
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - scraper-network

  # ───────────────────────────────────────
  load-balancer:
    image: nginx:latest
    container_name: load-balancer
    ports:
      - "8800:80"
    volumes:
      - ./config/nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./config/nginx/default.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - scrapyd1
      - scrapyd2
    networks:
      - scraper-network

  # API Gateway for automatic task distribution
  api-gateway:
    build:
      context: ./services/api-gateway
      dockerfile: Dockerfile
    container_name: api-gateway
    ports:
      - "5001:5000"
    depends_on:
      - scrapyd1
      - scrapyd2
      - redis
      - selenium-hub
      - proxy-rotator
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
      - MAX_SELENIUM_SESSIONS=4
      - SELENIUM_HUB_URL=http://selenium-hub:4444
      - PROXY_ROTATION_ENABLED=true
      - PROXY_SERVICE_URL=http://proxy-rotator:5000
      - DEFAULT_PROXY=http://tinyproxy1:8888
      - USER_AGENT_SERVICE_URL=http://ua-rotator:5000
    networks:
      - scraper-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5000/status" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ───────────────────────────────────────
  tinyproxy1:
    image: dannydirect/tinyproxy:latest
    container_name: tinyproxy1
    ports:
      - "8888:8888"
    command: [ "ANY" ] # This allows unrestricted access (for testing)
    networks:
      - scraper-network

  tinyproxy2:
    image: dannydirect/tinyproxy:latest
    container_name: tinyproxy2
    ports:
      - "8889:8888" # Different external port
    command: [ "ANY" ] # This allows unrestricted access (for testing)
    networks:
      - scraper-network

  # Redis for session management
  redis:
    image: redis:alpine
    container_name: scraper-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - scraper-network

  # MinIO S3 Storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000" # API
      - "9001:9001" # Console
    environment:
      - MINIO_ROOT_USER=minio_user
      - MINIO_ROOT_PASSWORD=minio_password
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    networks:
      - scraper-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3

  # MinIO bucket setup service (runs once and exits)
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c " sleep 5 && /usr/bin/mc config host add myminio http://minio:9000 minio_user minio_password && /usr/bin/mc mb --ignore-existing myminio/scraper-results && exit 0 "
    networks:
      - scraper-network

  # ───────────────────────────────────────
  # MESSAGE QUEUE INTEGRATION
  # ───────────────────────────────────────

  # RabbitMQ for task distribution
  rabbitmq:
    image: rabbitmq:3-management
    container_name: scraper-rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    environment:
      - RABBITMQ_DEFAULT_USER=guest
      - RABBITMQ_DEFAULT_PASS=guest
    networks:
      - scraper-network
    healthcheck:
      test: [ "CMD", "rabbitmqctl", "status" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Task processor service for consuming messages from RabbitMQ
  task-processor:
    build:
      context: ./services/task-processor
      dockerfile: Dockerfile
    container_name: task-processor
    depends_on:
      - rabbitmq
      - scrapyd1
      - scrapyd2
      - api-gateway
    environment:
      - RABBITMQ_HOST=rabbitmq
      - RABBITMQ_PORT=5672
      - RABBITMQ_USER=guest
      - RABBITMQ_PASSWORD=guest
      - RABBITMQ_QUEUE=scraper_tasks
      - API_GATEWAY_URL=http://api-gateway:5000
    networks:
      - scraper-network
    restart: unless-stopped

  # ───────────────────────────────────────
  # MONITORING AND LOGGING STACK
  # ───────────────────────────────────────

  # Elasticsearch for log storage
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.13.4
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms512m -Xmx512m
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - scraper-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # Kibana for log visualization
  kibana:
    image: docker.elastic.co/kibana/kibana:7.13.4
    container_name: kibana
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - scraper-network
    restart: unless-stopped

  # Filebeat for log collection
  filebeat:
    image: docker.elastic.co/beats/filebeat:7.13.4
    container_name: filebeat
    user: root
    volumes:
      - ./config/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./logs/scrapyd:/logs/scrapyd:ro # Add volume for scrapyd logs
    networks:
      - scraper-network
    depends_on:
      - elasticsearch
    restart: unless-stopped
    command: >
      bash -c "chown root:root /usr/share/filebeat/filebeat.yml &&
              chmod go-w /usr/share/filebeat/filebeat.yml &&
              filebeat -e"

  # Prometheus for metrics collection
  prometheus:
    image: prom/prometheus:v2.42.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./config/prometheus_rules.yml:/etc/prometheus/prometheus_rules.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=15d'
    networks:
      - scraper-network
    restart: unless-stopped

  # Grafana for metrics visualization
  grafana:
    build:
      context: ./docker/monitoring
      dockerfile: Dockerfile.grafana
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
      - elasticsearch
    networks:
      - scraper-network
    restart: unless-stopped

  # cAdvisor for container metrics
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.47.1
    container_name: cadvisor
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    ports:
      - "8080:8080"
    networks:
      - scraper-network
    restart: unless-stopped

  # Add the ua-rotator service to the docker-compose.yml file
  # This should be added as a service alongside the existing services
  ua-rotator:
    build:
      context: ./services/ua-rotator
      dockerfile: Dockerfile
    container_name: ua-rotator
    ports:
      - "5002:5000"
    volumes:
      - ua-rotator-data:/data
    environment:
      - PORT=5000
      - HOST=0.0.0.0
      - DATA_DIR=/data
      - DEBUG=false
    restart: unless-stopped
    networks:
      - scraper-network

  # Proxy rotation service
  proxy-rotator:
    build:
      context: ./services/proxy-rotator
      dockerfile: Dockerfile
    container_name: proxy-rotator
    ports:
      - "5003:5000"
    volumes:
      - proxy-rotator-data:/data
    environment:
      - PORT=5000
      - HOST=0.0.0.0
      - DATA_DIR=/data
      - DEBUG=false
      - PROXIES=http://tinyproxy1:8888,http://tinyproxy2:8888
    restart: unless-stopped
    networks:
      - scraper-network
    depends_on:
      - tinyproxy1
      - tinyproxy2

networks:
  scraper-network:
    driver: bridge

volumes:
  shared-eggs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./docker/shared-eggs
  postgres_data:
    driver: local
  minio_data:
    driver: local
  redis_data:
    driver: local
  elasticsearch_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  rabbitmq_data:
    driver: local
  # Add the volume for ua-rotator data persistence
  ua-rotator-data:
    driver: local
  proxy-rotator-data:
    driver: local
