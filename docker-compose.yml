services:
  scrapyd1:
    build:
      context: .
      dockerfile: Dockerfile.scrapyd
    container_name: scrapyd1
    environment:
      - SCRAPYD_EGGS_DIR=/root/.scrapyd/eggs
    ports:
      - "6800:6800"
    volumes:
      - ./shared-eggs:/root/.scrapyd/eggs
      - ./deploy_eggs.sh:/deploy_eggs.sh
      - ./data/cookies:/data/cookies # Volume for persistent cookies
    entrypoint: /deploy_eggs.sh
    networks:
      - scraper-network

  scrapyd2:
    build:
      context: .
      dockerfile: Dockerfile.scrapyd
    container_name: scrapyd2
    environment:
      - SCRAPYD_EGGS_DIR=/root/.scrapyd/eggs
    ports:
      - "6801:6800"
    volumes:
      - ./shared-eggs:/root/.scrapyd/eggs
      - ./deploy_eggs.sh:/deploy_eggs.sh
      - ./data/cookies:/data/cookies # Volume for persistent cookies
    entrypoint: /deploy_eggs.sh
    networks:
      - scraper-network

  scrapydweb:
    image: scrapydweb:latest
    container_name: scrapydweb
    depends_on:
      - scrapyd1
      - scrapyd2
    volumes:
      - ./scrapydweb_settings_v11.py:/app/scrapydweb_settings_v11.py
    ports:
      - "5000:5000"
    networks:
      - scraper-network

  # ───────────────────────────────────────
  selenium-hub:
    image: selenium/standalone-chrome:114.0
    container_name: selenium-hub
    ports:
      - "4444:4444"
    networks:
      - scraper-network
    environment:
      - SE_NODE_MAX_SESSIONS=4
      - SE_NODE_SESSION_TIMEOUT=300
      - SE_NODE_OVERRIDE_MAX_SESSIONS=true
    volumes:
      - ./data/cookies:/data/cookies # Volume for persistent cookies
    shm_size: 2gb

  # Database container to store scraping results
  postgres:
    image: postgres:15
    container_name: scraper-db
    restart: always
    environment:
      - POSTGRES_PASSWORD=scraper_password
      - POSTGRES_USER=scraper_user
      - POSTGRES_DB=scraper_data
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-db:/docker-entrypoint-initdb.d
    networks:
      - scraper-network

  # Optional: Database admin UI
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    environment:
      - PGADMIN_DEFAULT_EMAIL=admin@example.com
      - PGADMIN_DEFAULT_PASSWORD=admin
    ports:
      - "5050:80"
    depends_on:
      - postgres
    networks:
      - scraper-network

  # ───────────────────────────────────────
  load-balancer:
    image: nginx:latest
    container_name: load-balancer
    ports:
      - "8800:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - scrapyd1
      - scrapyd2
    networks:
      - scraper-network

  # API Gateway for automatic task distribution
  api-gateway:
    build:
      context: ./api-gateway
      dockerfile: Dockerfile
    container_name: api-gateway
    ports:
      - "5001:5000"
    depends_on:
      - scrapyd1
      - scrapyd2
      - redis
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=0
    networks:
      - scraper-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:5000/status" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped

  # ───────────────────────────────────────
  tinyproxy:
    image: dannydirect/tinyproxy:latest
    container_name: tinyproxy
    ports:
      - "8888:8888"
    command: [ "ANY" ] # This allows unrestricted access (for testing)
    networks:
      - scraper-network

  # Redis for session management
  redis:
    image: redis:alpine
    container_name: scraper-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - scraper-network

  # MinIO S3 Storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000" # API
      - "9001:9001" # Console
    environment:
      - MINIO_ROOT_USER=minio_user
      - MINIO_ROOT_PASSWORD=minio_password
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    networks:
      - scraper-network
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 30s
      timeout: 20s
      retries: 3

  # MinIO bucket setup service (runs once and exits)
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    depends_on:
      - minio
    entrypoint: >
      /bin/sh -c " sleep 5 && /usr/bin/mc config host add myminio http://minio:9000 minio_user minio_password && /usr/bin/mc mb --ignore-existing myminio/scraper-results && exit 0 "
    networks:
      - scraper-network

networks:
  scraper-network:
    driver: bridge

volumes:
  shared-eggs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ./shared-eggs
  postgres_data:
    driver: local
  minio_data:
    driver: local
  redis_data:
    driver: local
