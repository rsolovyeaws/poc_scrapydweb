version: '3.8'

services:
  scrapyd:
    image: scrapyd:latest
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.50'
          memory: 512M
    environment:
      - SCRAPY_PROJECT=demo
      - NODE_NAME={{.Task.Name}}
    volumes:
      - shared_eggs:/var/lib/scrapyd/eggs
      - shared_dbs:/var/lib/scrapyd/dbs
      - shared_items:/var/lib/scrapyd/items
    ports:
      - "6800-6801:6800"
    networks:
      - scraper-overlay
    depends_on:
      - selenium-hub

  scrapyd-web:
    image: scrapydweb:latest
    environment:
      - SCRAPYD_SERVERS=scrapyd_1:6800,scrapyd_2:6800
    volumes:
      - ./minimal_scrapydweb_settings.py:/app/scrapydweb_settings_v11.py
      - ./data:/app/data
    ports:
      - "5000:5000"
    networks:
      - scraper-overlay
    # Basic run command with no additional options
    entrypoint: ["/usr/local/bin/python", "-m", "scrapydweb"]
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      
  selenium-hub:
    image: selenium/standalone-chrome:114.0
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
    environment:
      - SE_NODE_MAX_SESSIONS=8
      - SE_NODE_SESSION_TIMEOUT=300
      - SE_NODE_OVERRIDE_MAX_SESSIONS=true
    ports:
      - "4444:4444"
    networks:
      - scraper-overlay
    # Using tmpfs instead of shm_size
    tmpfs:
      - /dev/shm:size=2g

  load-balancer:
    image: nginx:latest
    ports:
      - "8800:80"
    volumes:
      - ./nginx.conf:/etc/nginx/conf.d/default.conf
    depends_on:
      - scrapyd
    networks:
      - scraper-overlay

volumes:
  shared_eggs:
  shared_dbs:
  shared_items:

networks:
  scraper-overlay:
    driver: overlay